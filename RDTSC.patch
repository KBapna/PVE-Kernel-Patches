diff -Naur a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
--- a/arch/x86/kvm/vmx/vmx.c	2025-08-15 16:02:09.844571472 +0530
+++ b/arch/x86/kvm/vmx/vmx.c	2025-08-15 16:03:45.943078458 +0530
@@ -4464,11 +4464,13 @@
 	 * Not used by KVM, but fully supported for nesting, i.e. are allowed in
 	 * vmcs12 and propagated to vmcs02 when set in vmcs12.
 	 */
-	exec_control &= ~(CPU_BASED_RDTSC_EXITING |
-			  CPU_BASED_USE_IO_BITMAPS |
+        exec_control &= ~(CPU_BASED_USE_IO_BITMAPS |
 			  CPU_BASED_MONITOR_TRAP_FLAG |
 			  CPU_BASED_PAUSE_EXITING);
 
+        // Ensure handle_rdtsc() is used.
+        exec_control |= CPU_BASED_RDTSC_EXITING;
+
 	/* INTR_WINDOW_EXITING and NMI_WINDOW_EXITING are toggled dynamically */
 	exec_control &= ~(CPU_BASED_INTR_WINDOW_EXITING |
 			  CPU_BASED_NMI_WINDOW_EXITING);
@@ -4860,6 +4862,11 @@
 	vmx->msr_ia32_umwait_control = 0;
 
 	vmx->hv_deadline_tsc = -1;
+
+        vmx->rdtsc_clip.last_real  = 0;
+        vmx->rdtsc_clip.last_guest = 0;
+        vmx->rdtsc_clip.seq        = 0;
+
 	kvm_set_cr8(vcpu, 0);
 
 	vmx_segment_cache_clear(vmx);
@@ -6083,6 +6090,108 @@
 	return 1;
 }
 
+/* --- Smart RDTSC Clamping & Scaling Parameters --- */
+#define RDTSC_NOISE_SHIFT        4
+
+static u32 rdtsc_clamp_threshold     = 10240; // Host delta threshold
+static u32 rdtsc_clamp_min_inc       = 32;    // Min increment on clamp
+static u32 rdtsc_clamp_max_inc       = 2048;  // Max increment on clamp
+static u32 rdtsc_clamp_noise_mask    = 0xFF;  // Pseudo-random noise
+
+MODULE_PARM_DESC(rdtsc_clamp_threshold, "Delta in host TSC below which guest TSC gets clamped");
+MODULE_PARM_DESC(rdtsc_clamp_min_inc, "Minimum increment applied during clamping");
+MODULE_PARM_DESC(rdtsc_clamp_max_inc, "Maximum clamped increment before fallback");
+MODULE_PARM_DESC(rdtsc_clamp_noise_mask, "Bitmask to inject pseudo-random TSC jitter");
+module_param(rdtsc_clamp_threshold, uint, 0644);
+module_param(rdtsc_clamp_min_inc, uint, 0644);
+module_param(rdtsc_clamp_max_inc, uint, 0644);
+module_param(rdtsc_clamp_noise_mask, uint, 0644);
+
+/* --- Helper Functions --- */
+
+static inline u64 scale_guest_tsc(u64 tsc, u64 ratio)
+{
+    return (ratio == kvm_caps.default_tsc_scaling_ratio) ?
+           tsc : mul_u64_u64_shr(tsc, ratio, kvm_caps.tsc_scaling_ratio_frac_bits);
+}
+
+static inline void set_guest_tsc_regs(struct kvm_vcpu *vcpu, u64 val)
+{
+    vcpu->arch.regs[VCPU_REGS_RAX] = (u32)val;
+    vcpu->arch.regs[VCPU_REGS_RDX] = (u32)(val >> 32);
+}
+
+/* --- Core: Monotonic + Smooth Guest TSC Synthesizer --- */
+
+static u64 get_guest_tsc_val(struct kvm_vcpu *vcpu, u64 real_tsc)
+{
+    struct vcpu_vmx *vmx = to_vmx(vcpu);
+    u64 delta = real_tsc - vmx->rdtsc_clip.last_real;
+    u64 scaled_tsc, guest_tsc, catchup_target, noise;
+    u64 ratio = vcpu->arch.tsc_scaling_ratio;
+
+    if (vmx_get_cpl(vcpu) != 0 || !is_protmode(vcpu))
+        ratio >>= 3;
+
+    scaled_tsc = scale_guest_tsc(real_tsc, ratio) + vcpu->arch.tsc_offset;
+
+    if (unlikely(rdtsc_clamp_min_inc == 0))
+        rdtsc_clamp_min_inc = 1;
+
+    noise = ((real_tsc >> RDTSC_NOISE_SHIFT) & rdtsc_clamp_noise_mask);
+
+    if (delta <= rdtsc_clamp_threshold) {
+        vmx->rdtsc_clip.seq++;
+
+        u64 base_inc;
+        if (vmx->rdtsc_clip.seq & 1)
+            base_inc = rdtsc_clamp_min_inc + (noise >> 2);
+        else
+            base_inc = rdtsc_clamp_min_inc * 24 + noise;
+
+        if (base_inc > rdtsc_clamp_max_inc)
+            base_inc = rdtsc_clamp_max_inc;
+
+        guest_tsc = vmx->rdtsc_clip.last_guest + base_inc;
+    }
+    else if (delta <= rdtsc_clamp_threshold * 10) {
+        u64 ramp_step = rdtsc_clamp_max_inc + noise;
+        catchup_target = max(vmx->rdtsc_clip.last_guest + ramp_step, scaled_tsc);
+        guest_tsc = catchup_target;
+    }
+    else {
+        guest_tsc = scaled_tsc;
+        vmx->rdtsc_clip.seq = 0;
+    }
+
+    return guest_tsc;
+}
+
+/* --- Exit Handlers --- */
+
+static int handle_rdtsc_exit(struct kvm_vcpu *vcpu)
+{
+    struct vcpu_vmx *vmx = to_vmx(vcpu);
+    u64 real_tsc = rdtsc_ordered();
+    u64 guest_tsc = get_guest_tsc_val(vcpu, real_tsc);
+
+    vmx->rdtsc_clip.last_real  = real_tsc;
+    vmx->rdtsc_clip.last_guest = guest_tsc;
+
+    set_guest_tsc_regs(vcpu, guest_tsc);
+    return skip_emulated_instruction(vcpu);
+}
+
+static int handle_rdtscp_exit(struct kvm_vcpu *vcpu)
+{
+    vcpu->arch.regs[VCPU_REGS_RCX] = vmcs_read16(VIRTUAL_PROCESSOR_ID);
+    return handle_rdtsc_exit(vcpu);
+}
+
+static int handle_umwait(struct kvm_vcpu *vcpu) { return skip_emulated_instruction(vcpu); }
+static int handle_tpause(struct kvm_vcpu *vcpu) { return skip_emulated_instruction(vcpu); }
+
+
 /*
  * The exit handlers return 1 if the exit was handled fully and guest execution
  * may resume.  Otherwise they set the kvm_run parameter to indicate what needs
@@ -6141,6 +6250,10 @@
 	[EXIT_REASON_ENCLS]		      = handle_encls,
 	[EXIT_REASON_BUS_LOCK]                = handle_bus_lock_vmexit,
 	[EXIT_REASON_NOTIFY]		      = handle_notify,
+        [EXIT_REASON_RDTSC]                   = handle_rdtsc_exit,
+        [EXIT_REASON_RDTSCP]                  = handle_rdtscp_exit,
+        [EXIT_REASON_UMWAIT]                  = handle_umwait,
+        [EXIT_REASON_TPAUSE]                  = handle_tpause,
 };
 
 static const int kvm_vmx_max_exit_handlers =
diff -Naur a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
--- a/arch/x86/kvm/vmx/vmx.h	2025-08-15 16:02:09.844571472 +0530
+++ b/arch/x86/kvm/vmx/vmx.h	2025-08-15 16:03:10.792470199 +0530
@@ -365,6 +365,13 @@
 		DECLARE_BITMAP(read, MAX_POSSIBLE_PASSTHROUGH_MSRS);
 		DECLARE_BITMAP(write, MAX_POSSIBLE_PASSTHROUGH_MSRS);
 	} shadow_msr_intercept;
+
+        struct {
+            u64 last_real;
+            u64 last_guest;
+            u8 seq;
+        } rdtsc_clip;
+
 };
 
 struct kvm_vmx {
@@ -680,6 +687,8 @@
 int intel_pmu_create_guest_lbr_event(struct kvm_vcpu *vcpu);
 void vmx_passthrough_lbr_msrs(struct kvm_vcpu *vcpu);
 
+u64 kvm_vmx_get_clamped_tsc(struct kvm_vcpu *vcpu);
+
 static __always_inline unsigned long vmx_get_exit_qual(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
